{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bd1c33",
   "metadata": {},
   "source": [
    "## 작업 경로 확인 및 varinf의 Repository clone\n",
    "- 처음 1회만 수행\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6345160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcfbbda",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TransformersDataAugmentation'...\n",
      "remote: Enumerating objects: 49, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 49 (delta 12), reused 38 (delta 8), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (49/49), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/varinf/TransformersDataAugmentation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58768c48",
   "metadata": {},
   "source": [
    "## Install the trec datastests (not L2 datasets)\n",
    "- 경로를 src/utils까지 이동한 후 trec datasets install을 수행\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292eaa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils\n"
     ]
    }
   ],
   "source": [
    "%cd /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967602d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-05 12:48:51--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/TREC/train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 268193 (262K) [text/plain]\n",
      "Saving to: ‘datasets/trec/train.raw’\n",
      "\n",
      "datasets/trec/train 100%[===================>] 261.91K  1.63MB/s    in 0.2s    \n",
      "\n",
      "2021-06-05 12:48:52 (1.63 MB/s) - ‘datasets/trec/train.raw’ saved [268193/268193]\n",
      "\n",
      "--2021-06-05 12:48:52--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/TREC/dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29692 (29K) [text/plain]\n",
      "Saving to: ‘datasets/trec/dev.raw’\n",
      "\n",
      "datasets/trec/dev.r 100%[===================>]  29.00K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-06-05 12:48:53 (2.00 MB/s) - ‘datasets/trec/dev.raw’ saved [29692/29692]\n",
      "\n",
      "--2021-06-05 12:48:53--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/TREC/test.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19995 (20K) [text/plain]\n",
      "Saving to: ‘datasets/trec/test.raw’\n",
      "\n",
      "datasets/trec/test. 100%[===================>]  19.53K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-06-05 12:48:54 (1.95 MB/s) - ‘datasets/trec/test.raw’ saved [19995/19995]\n",
      "\n",
      "--2021-06-05 12:48:54--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/stsa.binary/train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 670802 (655K) [text/plain]\n",
      "Saving to: ‘datasets/stsa/train.raw’\n",
      "\n",
      "datasets/stsa/train 100%[===================>] 655.08K  2.57MB/s    in 0.2s    \n",
      "\n",
      "2021-06-05 12:48:55 (2.57 MB/s) - ‘datasets/stsa/train.raw’ saved [670802/670802]\n",
      "\n",
      "--2021-06-05 12:48:55--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/stsa.binary/dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 74994 (73K) [text/plain]\n",
      "Saving to: ‘datasets/stsa/dev.raw’\n",
      "\n",
      "datasets/stsa/dev.r 100%[===================>]  73.24K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2021-06-05 12:48:56 (902 KB/s) - ‘datasets/stsa/dev.raw’ saved [74994/74994]\n",
      "\n",
      "--2021-06-05 12:48:56--  https://raw.githubusercontent.com/1024er/cbert_aug/crayon/datasets/stsa.binary/test.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 195399 (191K) [text/plain]\n",
      "Saving to: ‘datasets/stsa/test.raw’\n",
      "\n",
      "datasets/stsa/test. 100%[===================>] 190.82K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-06-05 12:48:56 (1.33 MB/s) - ‘datasets/stsa/test.raw’ saved [195399/195399]\n",
      "\n",
      "--2021-06-05 12:48:57--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/train/seq.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 617079 (603K) [text/plain]\n",
      "Saving to: ‘datasets/snips/train.seq’\n",
      "\n",
      "datasets/snips/trai 100%[===================>] 602.62K  1.98MB/s    in 0.3s    \n",
      "\n",
      "2021-06-05 12:48:58 (1.98 MB/s) - ‘datasets/snips/train.seq’ saved [617079/617079]\n",
      "\n",
      "--2021-06-05 12:48:58--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/train/label\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 184532 (180K) [text/plain]\n",
      "Saving to: ‘datasets/snips/train.label’\n",
      "\n",
      "datasets/snips/trai 100%[===================>] 180.21K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-06-05 12:48:59 (1.23 MB/s) - ‘datasets/snips/train.label’ saved [184532/184532]\n",
      "\n",
      "--2021-06-05 12:48:59--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/valid/seq.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33629 (33K) [text/plain]\n",
      "Saving to: ‘datasets/snips/valid.seq’\n",
      "\n",
      "datasets/snips/vali 100%[===================>]  32.84K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-06-05 12:48:59 (1.85 MB/s) - ‘datasets/snips/valid.seq’ saved [33629/33629]\n",
      "\n",
      "--2021-06-05 12:48:59--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/valid/label\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9900 (9.7K) [text/plain]\n",
      "Saving to: ‘datasets/snips/valid.label’\n",
      "\n",
      "datasets/snips/vali 100%[===================>]   9.67K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-06-05 12:49:00 (3.89 MB/s) - ‘datasets/snips/valid.label’ saved [9900/9900]\n",
      "\n",
      "--2021-06-05 12:49:00--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/test/seq.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33121 (32K) [text/plain]\n",
      "Saving to: ‘datasets/snips/test.seq’\n",
      "\n",
      "datasets/snips/test 100%[===================>]  32.34K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-06-05 12:49:01 (1.98 MB/s) - ‘datasets/snips/test.seq’ saved [33121/33121]\n",
      "\n",
      "--2021-06-05 12:49:01--  https://raw.githubusercontent.com/MiuLab/SlotGated-SLU/master/data/snips/test/label\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10120 (9.9K) [text/plain]\n",
      "Saving to: ‘datasets/snips/test.label’\n",
      "\n",
      "datasets/snips/test 100%[===================>]   9.88K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-05 12:49:01 (23.8 MB/s) - ‘datasets/snips/test.label’ saved [10120/10120]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash download_and_prepare_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074762e",
   "metadata": {},
   "source": [
    "## Dependencies (jupyter ver.)\n",
    "- CMD를 기본으로 설치하되, 설치하는 방법은 README를 참고.\n",
    "- import 후 버전확인만 수행해볼 것.\n",
    "- 이것은 jupyter에서 실행하는 방법을 기록함(실행할 필요 없음)\n",
    "\n",
    "    * pytorch 1.5\n",
    "    * fairseq 9.0\n",
    "    * transformers 2.9\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfd767d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.5.0\n",
      "  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (603.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 603.7 MB 39 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.6.0%2Bcu92-cp37-cp37m-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=4.1.1\n",
      "  Downloading Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=43df149a665402b3f1c0966f4368abfdd3d7c2f31c277308302b8e9c5b76d17e\n",
      "  Stored in directory: /home/delabgpu/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built future\n",
      "Installing collected packages: numpy, future, torch, pillow, torchvision\n",
      "Successfully installed future-0.18.2 numpy-1.20.3 pillow-8.2.0 torch-1.5.0+cu92 torchvision-0.6.0+cu92\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01a14f62",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0+cu92\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2e9f8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq==0.9\n",
      "  Downloading fairseq-0.9.0.tar.gz (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from fairseq==0.9) (1.14.5)\n",
      "Collecting cython\n",
      "  Using cached Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: numpy in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from fairseq==0.9) (1.20.3)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
      "\u001b[K     |████████████████████████████████| 720 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from fairseq==0.9) (1.5.0+cu92)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from cffi->fairseq==0.9) (2.20)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: future in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from torch->fairseq==0.9) (0.18.2)\n",
      "Building wheels for collected packages: fairseq\n",
      "  Building wheel for fairseq (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/delabgpu/anaconda3/envs/tae/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-gm83btdm/fairseq_6f808fbf7fe6405fa273ba6f7ee0a1dd/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-gm83btdm/fairseq_6f808fbf7fe6405fa273ba6f7ee0a1dd/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-y_c4b0r4\n",
      "       cwd: /tmp/pip-install-gm83btdm/fairseq_6f808fbf7fe6405fa273ba6f7ee0a1dd/\n",
      "  Complete output (278 lines):\n",
      "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "  running bdist_wheel\n",
      "  /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    warnings.warn(msg.format('we could not find ninja.'))\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.7\n",
      "  creating build/lib.linux-x86_64-3.7/examples\n",
      "  copying examples/__init__.py -> build/lib.linux-x86_64-3.7/examples\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/bleu.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/options.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/pdb.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/__init__.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/binarizer.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/search.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/trainer.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/progress_bar.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/registry.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/meters.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/file_utils.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/utils.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/hub_utils.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.7/fairseq\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/generate.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/setup.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/train.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  copying fairseq_cli/score.py -> build/lib.linux-x86_64-3.7/fairseq_cli\n",
      "  creating build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/__init__.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  copying examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-3.7/examples/noisychannel\n",
      "  creating build/lib.linux-x86_64-3.7/examples/speech_recognition\n",
      "  copying examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition\n",
      "  copying examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition\n",
      "  copying examples/speech_recognition/infer.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition\n",
      "  creating build/lib.linux-x86_64-3.7/examples/speech_recognition/criterions\n",
      "  copying examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/criterions\n",
      "  copying examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/criterions\n",
      "  copying examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/criterions\n",
      "  copying examples/speech_recognition/criterions/CTC_loss.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/criterions\n",
      "  creating build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  copying examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  copying examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  copying examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  copying examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  copying examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/data\n",
      "  creating build/lib.linux-x86_64-3.7/examples/speech_recognition/models\n",
      "  copying examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/models\n",
      "  copying examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/models\n",
      "  copying examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/models\n",
      "  creating build/lib.linux-x86_64-3.7/examples/speech_recognition/tasks\n",
      "  copying examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/tasks\n",
      "  copying examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-3.7/examples/speech_recognition/tasks\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/logsumexp_moe.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/highway.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/mean_pool_gating_network.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-3.7/fairseq/modules\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/binary_cross_entropy.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.7/fairseq/criterions\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/truncate_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/sharded_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/levenshtein_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/wav2vec.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/insertion_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/cmlm_transformer.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.7/fairseq/models\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/translation_moe.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-3.7/fairseq/tasks\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-3.7/fairseq/optim\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/modules/dynamicconv_layer\n",
      "  copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-3.7/fairseq/modules/dynamicconv_layer\n",
      "  copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/modules/dynamicconv_layer\n",
      "  copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-3.7/fairseq/modules/dynamicconv_layer\n",
      "  copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.7/fairseq/modules/dynamicconv_layer\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/modules/lightconv_layer\n",
      "  copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/modules/lightconv_layer\n",
      "  copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-3.7/fairseq/modules/lightconv_layer\n",
      "  copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-3.7/fairseq/modules/lightconv_layer\n",
      "  copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.7/fairseq/modules/lightconv_layer\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/data/legacy\n",
      "  copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/data/legacy\n",
      "  copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data/legacy\n",
      "  copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data/legacy\n",
      "  copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-3.7/fairseq/data/legacy\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/data/audio\n",
      "  copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/data/audio\n",
      "  copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-3.7/fairseq/data/audio\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-3.7/fairseq/data/encoders\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/models/bart\n",
      "  copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/models/bart\n",
      "  copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-3.7/fairseq/models/bart\n",
      "  copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-3.7/fairseq/models/bart\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/models/roberta\n",
      "  copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/models/roberta\n",
      "  copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-3.7/fairseq/models/roberta\n",
      "  copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-3.7/fairseq/models/roberta\n",
      "  copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-3.7/fairseq/models/roberta\n",
      "  creating build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.7/fairseq/optim/lr_scheduler\n",
      "  running build_ext\n",
      "  building 'fairseq.libbleu' extension\n",
      "  creating build/temp.linux-x86_64-3.7\n",
      "  creating build/temp.linux-x86_64-3.7/fairseq\n",
      "  creating build/temp.linux-x86_64-3.7/fairseq/clib\n",
      "  creating build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
      "  gcc -pthread -B /home/delabgpu/anaconda3/envs/tae/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/delabgpu/anaconda3/envs/tae/include/python3.7m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -B /home/delabgpu/anaconda3/envs/tae/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/delabgpu/anaconda3/envs/tae/include/python3.7m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "  g++ -pthread -shared -B /home/delabgpu/anaconda3/envs/tae/compiler_compat -L/home/delabgpu/anaconda3/envs/tae/lib -Wl,-rpath=/home/delabgpu/anaconda3/envs/tae/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
      "  building 'fairseq.data.data_utils_fast' extension\n",
      "  error: unknown file type '.pyx' (from 'fairseq/data/data_utils_fast.pyx')\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for fairseq\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for fairseq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build fairseq\n",
      "Installing collected packages: portalocker, tqdm, sacrebleu, regex, cython, fairseq\n",
      "    Running setup.py install for fairseq ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: fairseq was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
      "\u001b[?25hSuccessfully installed cython-0.29.23 fairseq-0.9.0 portalocker-2.0.0 regex-2021.4.4 sacrebleu-1.5.1 tqdm-4.61.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96c5340a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "\n",
    "print(fairseq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c225563d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.9.0\n",
      "  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n",
      "\u001b[K     |████████████████████████████████| 635 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from transformers==2.9.0) (2021.4.4)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from transformers==2.9.0) (4.61.0)\n",
      "Requirement already satisfied: numpy in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from transformers==2.9.0) (1.20.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 5.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from requests->transformers==2.9.0) (2020.12.5)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from sacremoses->transformers==2.9.0) (1.16.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from click->sacremoses->transformers==2.9.0) (4.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.9.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.9.0) (3.10.0.0)\n",
      "Installing collected packages: urllib3, joblib, idna, click, chardet, tokenizers, sentencepiece, sacremoses, requests, filelock, transformers\n",
      "Successfully installed chardet-4.0.0 click-8.0.1 filelock-3.0.12 idna-2.10 joblib-1.0.1 requests-2.25.1 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.7.0 transformers-2.9.0 urllib3-1.26.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7b1841e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521add8f",
   "metadata": {},
   "source": [
    "## Prepare the L2-Datasets(or your datasets) and match data format - in README\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bab0a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4027e2",
   "metadata": {},
   "source": [
    "### Labeling and converting file format\n",
    "- 맨 처음에만 수행할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ef746d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-17b0f0a64853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlist1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "''' 1. Make label -> Description '''\n",
    "\n",
    "col=['Description']\n",
    "list1 = []\n",
    "\n",
    "for i in range(len(file)):\n",
    "    list1.append('Description')\n",
    "    \n",
    "df = pd.DataFrame(list1, columns = col)\n",
    "print(len(df))\n",
    "\n",
    "file = pd.read_csv('Datasets L2+L3+ETGEN.txt', delimiter = '\\t')\n",
    "print(len(file))\n",
    "\n",
    "result = pd.concat([df,file],axis=1, join='inner')\n",
    "\n",
    "\n",
    "''' 2. txt -> tsv 변환하여 저장 '''\n",
    "result.to_csv('L2_labeling_file.tsv', index=False, header=None, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18dca21",
   "metadata": {},
   "source": [
    "### L2_labeling_file.tsv 를 train dev test 분할\n",
    "- 맨 처음 한 번만 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8d660bd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/scripts\n",
      "/home/delabgpu/Tae_StudyCode\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "%cd /home/delabgpu/Tae_StudyCode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e973c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = pd.read_csv('L2_labeling_file.tsv', delimiter = '\\t')\n",
    "\n",
    "total_train = result[:264234]\n",
    "\n",
    "total_dev = result[264234:528468]\n",
    "\n",
    "total_test = result[528468:]\n",
    "\n",
    "\n",
    "%cd TransformersDataAugmentation/src/utils/datasets/L2\n",
    "\n",
    "total_train.to_csv('train.tsv', index=False, header=None, sep=\"\\t\")\n",
    "\n",
    "total_dev.to_csv('dev.tsv', index=False, header=None, sep=\"\\t\")\n",
    "\n",
    "total_test.to_csv('test.tsv', index=False, header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe11d61",
   "metadata": {},
   "source": [
    "### L2_labeling_file.tsv를 exp_{i}_10 폴더에 n등분 분할하여 배치\n",
    "- 전체를 n등분\n",
    "- 일부만 추출하여 n등분 (아래 코드는 이 방식을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73ada07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e3ca1c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n"
     ]
    }
   ],
   "source": [
    "%cd /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e8521",
   "metadata": {},
   "source": [
    "## train.tsv\n",
    "- 0 ~ (3523 * 15) = 0~52831   완료\n",
    "- 140,894 ~ (+ 3523 * 15)번째 = 140,894 ~ 193,725 완료\n",
    "- 281,787 ~ (+ 52831번째) = 281,787 ~ 334,618 완료\n",
    "- 422,680 ~ 475,511 완료\n",
    "- 563,573 ~ 616,404 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d6c4678",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  0 번째 성공 ]  ######\n",
      "마지막숫자 :  3523\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  1 번째 성공 ]  ######\n",
      "마지막숫자 :  7045\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  2 번째 성공 ]  ######\n",
      "마지막숫자 :  10567\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  3 번째 성공 ]  ######\n",
      "마지막숫자 :  14089\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  4 번째 성공 ]  ######\n",
      "마지막숫자 :  17611\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  5 번째 성공 ]  ######\n",
      "마지막숫자 :  21133\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  6 번째 성공 ]  ######\n",
      "마지막숫자 :  24655\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  7 번째 성공 ]  ######\n",
      "마지막숫자 :  28177\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  8 번째 성공 ]  ######\n",
      "마지막숫자 :  31699\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  9 번째 성공 ]  ######\n",
      "마지막숫자 :  35221\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  10 번째 성공 ]  ######\n",
      "마지막숫자 :  38743\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  11 번째 성공 ]  ######\n",
      "마지막숫자 :  42265\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  12 번째 성공 ]  ######\n",
      "마지막숫자 :  45787\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  13 번째 성공 ]  ######\n",
      "마지막숫자 :  49309\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  14 번째 성공 ]  ######\n",
      "마지막숫자 :  52831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 14번째 성공의 마지막 숫자 +1부터 dev만들기 '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" [ 첫 실행 시 < 전체에서 1/5만큼 > exp_j_10 / train.tsv 만들기 ] \"\"\"\n",
    "\n",
    "subset = []\n",
    "last_num=0\n",
    "\n",
    "for j in range(15):    \n",
    "    for i in range(3523):\n",
    "        subset = result[(j*i)+1:((j*i)+3523)]\n",
    "        last_num = ((j*i)+3523)\n",
    "    %cd exp_{j}_10\n",
    "    subset.to_csv('train.tsv', index=False, header=None, sep=\"\\t\")\n",
    "    %cd ../\n",
    "    print(\"#########  [ \", j, \"번째 성공 ]  ######\")\n",
    "    print(\"마지막숫자 : \", last_num)\n",
    "\n",
    "''' 14번째 성공의 마지막 숫자 +1부터 dev만들기 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6794a73",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  0 번째 성공 ]  ######\n",
      "마지막숫자 :  3523\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  1 번째 성공 ]  ######\n",
      "마지막숫자 :  7045\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  2 번째 성공 ]  ######\n",
      "마지막숫자 :  10567\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  3 번째 성공 ]  ######\n",
      "마지막숫자 :  14089\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  4 번째 성공 ]  ######\n",
      "마지막숫자 :  17611\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  5 번째 성공 ]  ######\n",
      "마지막숫자 :  21133\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  6 번째 성공 ]  ######\n",
      "마지막숫자 :  24655\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  7 번째 성공 ]  ######\n",
      "마지막숫자 :  28177\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  8 번째 성공 ]  ######\n",
      "마지막숫자 :  31699\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  9 번째 성공 ]  ######\n",
      "마지막숫자 :  35221\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  10 번째 성공 ]  ######\n",
      "마지막숫자 :  38743\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  11 번째 성공 ]  ######\n",
      "마지막숫자 :  42265\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  12 번째 성공 ]  ######\n",
      "마지막숫자 :  45787\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  13 번째 성공 ]  ######\n",
      "마지막숫자 :  49309\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  14 번째 성공 ]  ######\n",
      "마지막숫자 :  52831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 14번째 성공의 마지막 숫자 +1부터 dev만들기 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" [ 두 번째 이후부터 실행 시 < 전체에서 1/5 다음부터 1/5만큼 > exp_j_10 / train.tsv 만들기 ] \"\"\"\n",
    "\n",
    "result = result[140894+1:]\n",
    "last_num=0\n",
    "\n",
    "for j in range(15):    \n",
    "    for i in range(3523):\n",
    "        subset = result[(j*i)+1:((j*i)+3523)]\n",
    "        last_num = ((j*i)+3523)\n",
    "    %cd exp_{j}_10\n",
    "    subset.to_csv('train.tsv', index=False, header=None, sep=\"\\t\")\n",
    "    %cd ../\n",
    "    print(\"#########  [ \", j, \"번째 성공 ]  ######\")\n",
    "    print(\"마지막숫자 : \", last_num)\n",
    "\n",
    "''' 14번째 성공의 마지막 숫자 +1부터 dev만들기 '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01956cff",
   "metadata": {},
   "source": [
    "## dev.tsv\n",
    "- 52831 ~ (52831 + (3523 * 15)) = 52,831~105,662\n",
    "- 193,725 ~ 246,556 완료\n",
    "- 334,618 ~ 387,449 완료\n",
    "- 475,511 ~ 528342 완료\n",
    "- 616,404 ~ 669,235 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a773dc83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  0 번째 성공 ]  ######\n",
      "마지막숫자 :  3523\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  1 번째 성공 ]  ######\n",
      "마지막숫자 :  7045\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  2 번째 성공 ]  ######\n",
      "마지막숫자 :  10567\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  3 번째 성공 ]  ######\n",
      "마지막숫자 :  14089\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  4 번째 성공 ]  ######\n",
      "마지막숫자 :  17611\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  5 번째 성공 ]  ######\n",
      "마지막숫자 :  21133\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  6 번째 성공 ]  ######\n",
      "마지막숫자 :  24655\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  7 번째 성공 ]  ######\n",
      "마지막숫자 :  28177\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  8 번째 성공 ]  ######\n",
      "마지막숫자 :  31699\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  9 번째 성공 ]  ######\n",
      "마지막숫자 :  35221\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  10 번째 성공 ]  ######\n",
      "마지막숫자 :  38743\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  11 번째 성공 ]  ######\n",
      "마지막숫자 :  42265\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  12 번째 성공 ]  ######\n",
      "마지막숫자 :  45787\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  13 번째 성공 ]  ######\n",
      "마지막숫자 :  49309\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  14 번째 성공 ]  ######\n",
      "마지막숫자 :  52831\n"
     ]
    }
   ],
   "source": [
    "\"\"\" [ < 전체에서 1/5만큼 > exp_j_10 / dev.tsv 만들기 ] \"\"\"\n",
    "\n",
    "result = result[last_num+1:]\n",
    "subset = []\n",
    "\n",
    "for j in range(15):    \n",
    "    for i in range(3523):\n",
    "        subset = result[(j*i)+1:((j*i)+3523)]\n",
    "        last_num = ((j*i)+3523)\n",
    "    %cd exp_{j}_10\n",
    "    subset.to_csv('dev.tsv', index=False, header=None, sep=\"\\t\")\n",
    "    %cd ../\n",
    "    print(\"#########  [ \", j, \"번째 성공 ]  ######\")\n",
    "    print(\"마지막숫자 : \", last_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d91bea77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc04b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode\n"
     ]
    }
   ],
   "source": [
    "%cd /home/delabgpu/Tae_StudyCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581231d",
   "metadata": {},
   "source": [
    "## test.tsv\n",
    "- 105,662 ~ (105,662 + 35231) = 105,662 ~ 140,893\n",
    "- 246,556 ~ 281,787 완료\n",
    "- 387,449 ~ 422,680 완료\n",
    "- 528,342 ~ 563,573 완료\n",
    "- 669,235 ~ 704,466 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80bef09f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  0 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  1 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  2 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  3 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  4 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  5 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  6 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  7 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  8 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  9 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  10 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  11 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  12 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  13 번째 성공 ]  ######\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10\n",
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\n",
      "#########  [  14 번째 성공 ]  ######\n"
     ]
    }
   ],
   "source": [
    "''' 각 exp에 test.tsv 넣어줌 '''\n",
    "\n",
    "result = result[last_num+1:]\n",
    "test_result = result[:35231]   # 나중에 할 땐 범위 바꿔줘야함\n",
    "\n",
    "%cd TransformersDataAugmentation/src/utils/datasets/L2\n",
    "\n",
    "for j in range(15):        \n",
    "    %cd exp_{j}_10\n",
    "    test_result.to_csv('test.tsv', index=False, header=None, sep=\"\\t\")\n",
    "    %cd ../\n",
    "    print(\"#########  [ \", j, \"번째 성공 ]  ######\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a14f89",
   "metadata": {},
   "source": [
    "## Augmentation 수행 전, 경로 수정\n",
    "1. /src/scripts/bert_L2_lower.sh 파일을 열어서 경로 수정 (아래 주석과 README파일 참고)\n",
    "2. Datasets 폴더에 \"L2\"폴더를 생성한 후, exp_{0~14}_10 폴더를 만듦 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SRC=/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src\n",
    "CACHE=/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE\n",
    "TASK=trec\n",
    "\n",
    "for NUMEXAMPLES in 10;\n",
    "do\n",
    "    for i in {0..14};\n",
    "        do\n",
    "        RAWDATADIR=/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_${i}_${NUMEXAMPLES}\n",
    "\n",
    "       # Baseline classifier\n",
    "        python $SRC/bert_aug/bert_classifier.py --task $TASK  --data_dir $RAWDATADIR --seed ${i} --learning_rate $BERTLR --cache $CACHE > $RAWDATADIR/bert_baseline.log\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e82b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf3ec3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0aeac9",
   "metadata": {},
   "source": [
    "## Augmentation 수행\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "366d3004",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10/bt', sample_num=1, seed=0, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_0_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fd6e9ac0536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fd71a07f183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fd71a081b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fd719ecfb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fd71a2fbdb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fd719ed82d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fd71a2e276f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fd71a307c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fd71bce2ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fd71a307c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fd71baea787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fd71bfcdc05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fd71bfcaf03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fd71bfcbce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fd71bfc4359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fd728702cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fd7292eae0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fd72b4fa6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fd72b2304dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/17/2021 14:14:37 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/17/2021 14:14:38 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/17/2021 14:14:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17/2021 14:14:38 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/17/2021 14:14:42 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/17/2021 14:14:42 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   guid: train-0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   tokens: [CLS] description the park opened in and it is still a draw for tourists . [SEP]\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   init_ids: 101 6412 1996 2380 2441 1999 1998 2009 2003 2145 1037 4009 2005 9045 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   input_ids: 101 6412 1996 103 2441 1999 1998 2009 2003 2145 1037 4009 103 9045 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 2380 -100 -100 -100 -100 -100 -100 -100 -100 2005 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   guid: train-1\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   tokens: [CLS] description the first loop ##ing roller coaster the first loop ##ing coaster was built in paris in this cent ##ri ##fu ##gal railway offered a rail car hat would travel through the loop with nothing ee ##ping it there aside from its own cent ##rip ##eta ##l acceleration . [SEP]\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   init_ids: 101 6412 1996 2034 7077 2075 11220 16817 1996 2034 7077 2075 16817 2001 2328 1999 3000 1999 2023 9358 3089 11263 9692 2737 3253 1037 4334 2482 6045 2052 3604 2083 1996 7077 2007 2498 25212 4691 2009 2045 4998 2013 2049 2219 9358 29443 12928 2140 16264 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   input_ids: 101 6412 1996 2034 7077 2075 11220 103 1996 2034 7077 2075 16817 2001 2328 1999 3000 1999 2023 9358 3089 11263 9692 2737 3253 1037 4334 103 1999 2052 3604 2083 1996 7077 103 2498 25212 4691 103 2045 4998 2013 2049 103 9358 103 12928 2140 2737 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 16817 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2482 6045 -100 -100 -100 -100 -100 2007 -100 -100 -100 2009 -100 -100 -100 -100 2219 -100 29443 -100 -100 16264 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 14:14:42 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   guid: dev-0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   tokens: [CLS] description i ' m reading my history textbook . [SEP]\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   init_ids: 101 6412 1045 1005 1049 3752 2026 2381 16432 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   input_ids: 101 6412 1045 103 1049 3752 103 2381 16432 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 1005 -100 -100 2026 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   guid: dev-1\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   tokens: [CLS] description i ' m going to dance at the school festival . [SEP]\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   init_ids: 101 6412 1045 1005 1049 2183 2000 3153 2012 1996 2082 2782 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   input_ids: 101 6412 1045 1005 1049 2183 2000 3153 1996 1996 2082 2782 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 2012 -100 -100 2782 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 14:14:44 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   ********************************\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   ********************************\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   *  Batch size = 8\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/17/2021 14:14:45 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:40<00:00, 820.08s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f8478fee536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f84a95ad183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f84a95afb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f84a93fdb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f84a9829db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f84a94062d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f84a981076f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f84a9835c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f84ab210ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f84a9835c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f84ab018787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f84ab4fbc05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f84ab4f8f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f84ab4f9ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f84ab4f2359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f84b7c30cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f84b8818e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f84baa286ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f84ba75e4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10/bt', sample_num=1, seed=1, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_1_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fa4003c8536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fa430987183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fa430989b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fa4307d7b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fa430c03db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fa4307e02d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fa430bea76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fa430c0fc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fa4325eaceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fa430c0fc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fa4323f2787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fa4328d5c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fa4328d2f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fa4328d3ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fa4328cc359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fa43f00acf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fa43fbf2e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fa441e026ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fa441b384dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/17/2021 16:35:42 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/17/2021 16:35:43 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/17/2021 16:35:43 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17/2021 16:35:43 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/17/2021 16:35:47 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/17/2021 16:35:47 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   guid: train-0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   tokens: [CLS] description i ' m not sure how to handle this kind of situation , though . [SEP]\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   init_ids: 101 6412 1045 1005 1049 2025 2469 2129 2000 5047 2023 2785 1997 3663 1010 2295 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   input_ids: 101 6412 103 1005 1049 2025 2469 2129 2000 5047 2023 2785 1997 3663 1010 2295 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   masked_lm_labels: -100 -100 1045 -100 -100 -100 -100 -100 -100 -100 -100 -100 1997 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   guid: train-1\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   tokens: [CLS] description before you say anything , try to think twice . [SEP]\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   init_ids: 101 6412 2077 2017 2360 2505 1010 3046 2000 2228 3807 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   input_ids: 101 6412 2077 2017 103 103 1010 3046 2000 2228 3807 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2360 2505 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 16:35:47 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   guid: dev-0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   tokens: [CLS] description what ' s this buzzing in my ear ? [SEP]\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   init_ids: 101 6412 2054 1005 1055 2023 20386 1999 2026 4540 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   input_ids: 101 6412 2054 1005 1055 2023 20386 103 103 4540 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 1999 2026 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   guid: dev-1\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   tokens: [CLS] description it ' ll give me energy for the next day . [SEP]\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   init_ids: 101 6412 2009 1005 2222 2507 2033 2943 2005 1996 2279 2154 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   input_ids: 101 6412 2009 1005 2222 2507 2033 2943 2005 103 2279 2154 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 1996 2279 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 16:35:49 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   ********************************\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   ********************************\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   *  Batch size = 8\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/17/2021 16:35:50 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:18:35<00:00, 831.59s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f7ec542a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f7ef59e9183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f7ef59ebb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f7ef5839b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f7ef5c65db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f7ef58422d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f7ef5c4c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f7ef5c71c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f7ef764cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f7ef5c71c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f7ef7454787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f7ef7937c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f7ef7934f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f7ef7935ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f7ef792e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f7f0406ccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f7f04c54e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f7f06e646ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f7f06b9a4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10/bt', sample_num=1, seed=2, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_2_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fc384d5a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fc3b5319183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fc3b531bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fc3b5169b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fc3b5595db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fc3b51722d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fc3b557c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fc3b55a1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fc3b6f7cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fc3b55a1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fc3b6d84787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fc3b7267c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fc3b7264f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fc3b7265ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fc3b725e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fc3c399ccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fc3c4584e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fc3c67946ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fc3c64ca4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/17/2021 18:58:47 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/17/2021 18:58:48 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/17/2021 18:58:48 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17/2021 18:58:48 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/17/2021 18:58:52 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/17/2021 18:58:52 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   guid: train-0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   tokens: [CLS] description i fed her and cleaned her cage every day . [SEP]\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   init_ids: 101 6412 1045 7349 2014 1998 12176 2014 7980 2296 2154 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   input_ids: 101 6412 1045 7349 2014 1998 12176 103 7980 2296 2154 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 2014 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   guid: train-1\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   tokens: [CLS] description it seems to me at ##ing this is killing her twice . [SEP]\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   init_ids: 101 6412 2009 3849 2000 2033 2012 2075 2023 2003 4288 2014 3807 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   input_ids: 101 6412 2009 103 2000 2033 2012 2075 2023 2012 4288 2014 3807 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 3849 -100 -100 -100 -100 -100 2003 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 18:58:52 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   guid: dev-0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   tokens: [CLS] description wow , i did n ' t know that bees are uc ##h an important part of our ecosystem . [SEP]\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   init_ids: 101 6412 10166 1010 1045 2106 1050 1005 1056 2113 2008 13734 2024 15384 2232 2019 2590 2112 1997 2256 16927 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   input_ids: 101 6412 10166 1010 1045 2106 1050 1005 1997 15384 2008 13734 2024 15384 2232 2019 2590 2112 1997 103 16927 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 1056 2113 -100 -100 -100 -100 -100 -100 -100 -100 -100 2256 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   guid: dev-1\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   tokens: [CLS] description bees lay important roles in helping plants to reproduce . [SEP]\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   init_ids: 101 6412 13734 3913 2590 4395 1999 5094 4264 2000 21376 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   input_ids: 101 6412 13734 103 2590 4395 1999 5094 4264 2000 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 3913 -100 -100 -100 -100 -100 -100 21376 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 18:58:53 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   ********************************\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   ********************************\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   *  Batch size = 8\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/17/2021 18:58:55 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:26:44<00:00, 880.46s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f4dc0ac8536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f4df1087183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f4df1089b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f4df0ed7b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f4df1303db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f4df0ee02d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f4df12ea76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f4df130fc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f4df2ceaceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f4df130fc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f4df2af2787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f4df2fd5c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f4df2fd2f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f4df2fd3ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f4df2fcc359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f4dff70acf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f4e002f2e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f4e025026ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f4e022384dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10/bt', sample_num=1, seed=3, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_3_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fa9c609b536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fa9f665a183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fa9f665cb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fa9f64aab93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fa9f68d6db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fa9f64b32d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fa9f68bd76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fa9f68e2c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fa9f82bdceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fa9f68e2c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fa9f80c5787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fa9f85a8c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fa9f85a5f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fa9f85a6ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fa9f859f359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7faa04cddcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7faa058c5e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7faa07ad56ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7faa0780b4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/17/2021 21:30:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/17/2021 21:30:15 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/17/2021 21:30:15 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17/2021 21:30:15 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/17/2021 21:30:19 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/17/2021 21:30:19 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   guid: train-0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   tokens: [CLS] description so i made up percy jackson and it o ##ok about three nights to tell the whole story . [SEP]\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   init_ids: 101 6412 2061 1045 2081 2039 11312 4027 1998 2009 1051 6559 2055 2093 6385 2000 2425 1996 2878 2466 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   input_ids: 101 6412 2061 1045 2081 103 11312 4027 103 2009 103 6559 2055 2093 6385 2000 2425 1996 2878 2466 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 2039 -100 -100 1998 -100 1051 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   guid: train-1\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   tokens: [CLS] description when i was done , my son told me i should write it out as a book . [SEP]\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   init_ids: 101 6412 2043 1045 2001 2589 1010 2026 2365 2409 2033 1045 2323 4339 2009 2041 2004 1037 2338 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   input_ids: 101 6412 2043 1045 2001 2589 1010 103 2365 2409 2033 1045 2323 4339 2009 2041 2004 1037 2338 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 2026 -100 -100 -100 1045 -100 -100 -100 -100 -100 1037 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 21:30:19 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   guid: dev-0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   tokens: [CLS] description how are you doing ? [SEP]\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   init_ids: 101 6412 2129 2024 2017 2725 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   input_ids: 101 6412 103 2024 2017 2725 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   masked_lm_labels: -100 -100 2129 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   guid: dev-1\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   tokens: [CLS] description good enough so you won ##t drown . [SEP]\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   init_ids: 101 6412 2204 2438 2061 2017 2180 2102 19549 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   input_ids: 101 6412 2204 2438 2061 2017 2180 103 19549 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 2102 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 21:30:20 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   ********************************\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   ********************************\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   *  Batch size = 8\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/17/2021 21:30:21 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:21<00:00, 818.16s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fc07975a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fc0a9d19183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fc0a9d1bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fc0a9b69b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fc0a9f95db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fc0a9b722d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fc0a9f7c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fc0a9fa1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fc0ab97cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fc0a9fa1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fc0ab784787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fc0abc67c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fc0abc64f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fc0abc65ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fc0abc5e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fc0b839ccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fc0b8f84e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fc0bb1946ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fc0baeca4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10/bt', sample_num=1, seed=4, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_4_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f9212f13536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f92434d2183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f92434d4b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f9243322b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f924374edb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f924332b2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f924373576f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f924375ac76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f9245135ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f924375ac76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f9244f3d787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f9245420c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f924541df03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f924541ece2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f9245417359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f9251b55cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f925273de0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f925494d6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f92546834dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/17/2021 23:51:04 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/17/2021 23:51:05 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/17/2021 23:51:05 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17/2021 23:51:05 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/17/2021 23:51:09 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/17/2021 23:51:09 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   guid: train-0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   tokens: [CLS] description special talents or i have a talent for drawing cartoons . [SEP]\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   init_ids: 101 6412 2569 11725 2030 1045 2031 1037 5848 2005 5059 13941 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   input_ids: 101 6412 2569 11725 103 1045 2031 103 5848 2005 5059 13941 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2030 -100 -100 1037 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   guid: train-1\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   tokens: [CLS] description since i was young , i have drawn thousands of cartoons and won many prizes in several contests . [SEP]\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   init_ids: 101 6412 2144 1045 2001 2402 1010 1045 2031 4567 5190 1997 13941 1998 2180 2116 11580 1999 2195 15795 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   input_ids: 101 6412 2144 1045 2001 2402 1010 103 2031 4567 103 1997 13941 1998 2180 2116 11580 1999 2195 15795 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 1045 -100 -100 5190 1997 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 23:51:09 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   guid: dev-0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   tokens: [CLS] description its making try this spring roll . [SEP]\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   init_ids: 101 6412 2049 2437 3046 2023 3500 4897 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   input_ids: 101 6412 2049 2437 3046 2023 103 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 3500 4897 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   *** Example ***\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   guid: dev-1\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   tokens: [CLS] description oh ! [SEP]\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   init_ids: 101 6412 2821 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   input_ids: 101 6412 2821 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   masked_lm_labels: -100 -100 2821 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/17/2021 23:51:11 - INFO - __main__ -   label_len: 1\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   ********************************\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   ********************************\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   *  Batch size = 8\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/17/2021 23:51:12 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:32<00:00, 819.22s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7efd63663536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7efd93c22183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7efd93c24b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7efd93a72b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7efd93e9edb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7efd93a7b2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7efd93e8576f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7efd93eaac76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7efd95885ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7efd93eaac76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7efd9568d787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7efd95b70c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7efd95b6df03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7efd95b6ece2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7efd95b67359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7efda22a5cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7efda2e8de0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7efda509d6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7efda4dd34dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10/bt', sample_num=1, seed=5, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_5_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7efe8970a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7efeb9cc9183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7efeb9ccbb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7efeb9b19b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7efeb9f45db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7efeb9b222d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7efeb9f2c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7efeb9f51c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7efebb92cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7efeb9f51c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7efebb734787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7efebbc17c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7efebbc14f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7efebbc15ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7efebbc0e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7efec834ccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7efec8f34e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7efecb1446ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7efecae7a4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 02:12:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 02:12:03 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 02:12:03 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 02:12:03 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 02:12:07 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 02:12:07 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   tokens: [CLS] description i ' m on my way home . [SEP]\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   init_ids: 101 6412 1045 1005 1049 2006 2026 2126 2188 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   input_ids: 101 6412 1045 1005 1049 2006 2026 2126 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 2006 -100 -100 2188 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   tokens: [CLS] description how did your basketball game go today ? [SEP]\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   init_ids: 101 6412 2129 2106 2115 3455 2208 2175 2651 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   input_ids: 101 6412 2129 2106 2115 3455 103 103 2651 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 2208 2175 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 02:12:07 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   tokens: [CLS] description look after according to name a after ##rem ##ind a of ##take over civil war on the other hand fight for early settler such as hold back originate from long face oh , you ' re talking about pi ##ran ##has , are n ' t you ? [SEP]\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   init_ids: 101 6412 2298 2044 2429 2000 2171 1037 2044 28578 22254 1037 1997 15166 2058 2942 2162 2006 1996 2060 2192 2954 2005 2220 18556 2107 2004 2907 2067 21754 2013 2146 2227 2821 1010 2017 1005 2128 3331 2055 14255 5521 14949 1010 2024 1050 1005 1056 2017 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   input_ids: 101 6412 2298 2044 2429 2000 2171 1037 2044 28578 22254 103 1997 2171 2058 2942 2162 2006 1996 2060 2192 2954 2005 103 18556 2107 2004 2907 103 21754 2013 2146 2227 2821 2000 2017 103 2128 3331 2055 14255 5521 14949 1010 2024 103 1005 1056 2017 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 1037 -100 15166 -100 -100 -100 -100 -100 -100 -100 -100 -100 2220 -100 -100 -100 -100 2067 -100 -100 -100 -100 -100 1010 -100 1005 -100 -100 -100 -100 -100 14949 -100 -100 1050 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   tokens: [CLS] description i like your idea . [SEP]\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   init_ids: 101 6412 1045 2066 2115 2801 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   input_ids: 101 6412 103 2066 2115 2801 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   masked_lm_labels: -100 -100 1045 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 02:12:09 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   ********************************\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   ********************************\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 02:12:10 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:18:28<00:00, 830.87s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f3b6bc66536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f3b9c225183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f3b9c227b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f3b9c075b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f3b9c4a1db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f3b9c07e2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f3b9c48876f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f3b9c4adc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f3b9de88ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f3b9c4adc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f3b9dc90787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f3b9e173c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f3b9e170f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f3b9e171ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f3b9e16a359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f3baa8a8cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f3bab490e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f3bad6a06ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f3bad3d64dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10/bt', sample_num=1, seed=6, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_6_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fb5109dc536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fb540f9b183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fb540f9db64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fb540debb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fb541217db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fb540df42d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fb5411fe76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fb541223c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fb542bfeceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fb541223c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fb542a06787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fb542ee9c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fb542ee6f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fb542ee7ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fb542ee0359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fb54f61ecf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fb550206e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fb5524166ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fb55214c4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 04:34:55 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 04:34:56 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 04:34:56 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 04:34:56 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 04:35:00 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 04:35:00 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   tokens: [CLS] description um , it looks great , except for some words like blind and deaf . [SEP]\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   init_ids: 101 6412 8529 1010 2009 3504 2307 1010 3272 2005 2070 2616 2066 6397 1998 12419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   input_ids: 101 6412 8529 1010 2009 3504 2307 1010 103 2005 1010 2616 2066 6397 1998 12419 2070 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 3272 -100 2070 -100 -100 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   tokens: [CLS] description is there something wrong with using those words ? [SEP]\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   init_ids: 101 6412 2003 2045 2242 3308 2007 2478 2216 2616 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   input_ids: 101 6412 2003 2045 103 3308 2007 2478 103 2616 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2242 -100 -100 -100 2216 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 04:35:00 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   tokens: [CLS] description go ahead . [SEP]\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   init_ids: 101 6412 2175 3805 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   input_ids: 101 6412 2175 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 3805 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   tokens: [CLS] description i would like you to read it . [SEP]\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   init_ids: 101 6412 1045 2052 2066 2017 2000 3191 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   input_ids: 101 6412 1045 2052 2066 2017 2000 103 2009 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 3191 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 04:35:02 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   ********************************\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   ********************************\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 04:35:03 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:15:53<00:00, 815.31s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f187180b536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f18a1dca183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f18a1dccb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f18a1c1ab93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f18a2046db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f18a1c232d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f18a202d76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f18a2052c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f18a3a2dceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f18a2052c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f18a3835787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f18a3d18c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f18a3d15f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f18a3d16ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f18a3d0f359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f18b044dcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f18b1035e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f18b32456ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f18b2f7b4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10/bt', sample_num=1, seed=7, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_7_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f2908ffb536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f29395ba183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f29395bcb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f293940ab93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f2939836db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f29394132d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f293981d76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f2939842c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f293b21dceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f2939842c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f293b025787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f293b508c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f293b505f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f293b506ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f293b4ff359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f2947c3dcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f2948825e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f294aa356ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f294a76b4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 06:55:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 06:55:15 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 06:55:15 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 06:55:15 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 06:55:19 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 06:55:19 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   tokens: [CLS] description but it might be a soft drink . [SEP]\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   init_ids: 101 6412 2021 2009 2453 2022 1037 3730 4392 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   input_ids: 101 6412 2021 2009 2453 2022 1037 3730 103 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 4392 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   tokens: [CLS] description could be . [SEP]\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   init_ids: 101 6412 2071 2022 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   input_ids: 101 6412 2071 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 2022 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 06:55:19 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   tokens: [CLS] description something that you own but others use more than you a name others call your name more often than you . [SEP]\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   init_ids: 101 6412 2242 2008 2017 2219 2021 2500 2224 2062 2084 2017 1037 2171 2500 2655 2115 2171 2062 2411 2084 2017 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   input_ids: 101 6412 2242 2008 2017 2219 2021 2500 2224 103 2084 2017 1037 2171 2500 2655 2242 2171 2500 2411 2084 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 2062 -100 -100 -100 -100 -100 -100 2115 -100 2062 -100 -100 2017 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   tokens: [CLS] description the correct answer is smiles . [SEP]\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   init_ids: 101 6412 1996 6149 3437 2003 8451 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   input_ids: 101 6412 1996 6149 3437 2003 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 8451 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 06:55:20 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   ********************************\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   ********************************\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 06:55:22 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:50<00:00, 821.01s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fd18017a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fd1b0739183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fd1b073bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fd1b0589b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fd1b09b5db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fd1b05922d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fd1b099c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fd1b09c1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fd1b239cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fd1b09c1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fd1b21a4787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fd1b2687c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fd1b2684f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fd1b2685ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fd1b267e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fd1bedbccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fd1bf9a4e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fd1c1bb46ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fd1c18ea4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10/bt', sample_num=1, seed=8, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_8_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f93ead3d536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f941b2fc183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f941b2feb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f941b14cb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f941b578db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f941b1552d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f941b55f76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f941b584c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f941cf5fceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f941b584c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f941cd67787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f941d24ac05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f941d247f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f941d248ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f941d241359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f942997fcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f942a567e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f942c7776ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f942c4ad4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 09:16:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 09:16:28 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 09:16:28 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 09:16:28 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 09:16:32 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 09:16:32 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   tokens: [CLS] description i have been practicing a few basin painting skills for two months now , and i ' m still not getting it . [SEP]\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   init_ids: 101 6412 1045 2031 2042 12560 1037 2261 6403 4169 4813 2005 2048 2706 2085 1010 1998 1045 1005 1049 2145 2025 2893 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   input_ids: 101 6412 1045 2031 2042 12560 1037 2261 6403 4169 4813 103 2048 2706 2085 1010 1998 103 103 103 2145 2025 2893 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2005 -100 -100 -100 -100 -100 1045 1005 1049 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   tokens: [CLS] description why ca n ' t i even paint a simple picture yet ? [SEP]\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   init_ids: 101 6412 2339 6187 1050 1005 1056 1045 2130 6773 1037 3722 3861 2664 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   input_ids: 101 6412 2339 6187 1050 1005 1056 103 2130 6773 1037 3722 3861 2664 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 1045 -100 -100 -100 -100 -100 -100 1029 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 09:16:32 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   tokens: [CLS] description oh , thank you ! [SEP]\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   init_ids: 101 6412 2821 1010 4067 2017 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   input_ids: 101 6412 2821 1010 4067 103 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 2017 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   tokens: [CLS] description thank you so much for being my tour guide . [SEP]\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   init_ids: 101 6412 4067 2017 2061 2172 2005 2108 2026 2778 5009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   input_ids: 101 6412 4067 2017 2061 2172 2005 103 2026 2778 5009 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 2108 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 09:16:34 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   ********************************\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   ********************************\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 09:16:35 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:44<00:00, 820.40s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fda63e0d536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fda943cc183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fda943ceb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fda9421cb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fda94648db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fda942252d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fda9462f76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fda94654c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fda9602fceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fda94654c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fda95e37787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fda9631ac05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fda96317f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fda96318ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fda96311359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fdaa2a4fcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fdaa3637e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fdaa58476ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fdaa557d4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10/bt', sample_num=1, seed=9, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_9_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f5d1e32f536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f5d4e8ee183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f5d4e8f0b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f5d4e73eb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f5d4eb6adb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f5d4e7472d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f5d4eb5176f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f5d4eb76c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f5d50551ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f5d4eb76c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f5d50359787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f5d5083cc05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f5d50839f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f5d5083ace2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f5d50833359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f5d5cf71cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f5d5db59e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f5d5fd696ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f5d5fa9f4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 11:37:38 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 11:37:39 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 11:37:39 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 11:37:39 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 11:37:43 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 11:37:43 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   tokens: [CLS] description no . [SEP]\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   init_ids: 101 6412 2053 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   input_ids: 101 6412 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   masked_lm_labels: -100 -100 2053 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   tokens: [CLS] description thanks . [SEP]\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   init_ids: 101 6412 4283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   input_ids: 101 6412 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   masked_lm_labels: -100 -100 4283 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 11:37:43 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   tokens: [CLS] description actually , i ' m kind of sick of their burger ##s , but i still have some coup ##ons left . [SEP]\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   init_ids: 101 6412 2941 1010 1045 1005 1049 2785 1997 5305 1997 2037 15890 2015 1010 2021 1045 2145 2031 2070 8648 5644 2187 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   input_ids: 101 6412 2941 1010 103 1005 1049 2785 103 5305 1997 2037 15890 2015 103 2021 1045 2145 2031 103 8648 5644 2187 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 1045 -100 -100 -100 1997 -100 -100 -100 -100 -100 1010 -100 -100 -100 -100 2070 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   tokens: [CLS] description take advantage of such as after all at the forefront in conclusion to name a few get rid of have an influence on tend to at the same time you like to go to star burger ? [SEP]\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   init_ids: 101 6412 2202 5056 1997 2107 2004 2044 2035 2012 1996 22870 1999 7091 2000 2171 1037 2261 2131 9436 1997 2031 2019 3747 2006 7166 2000 2012 1996 2168 2051 2017 2066 2000 2175 2000 2732 15890 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   input_ids: 101 6412 2202 5056 1997 2107 2004 2044 2035 2012 1996 2004 1999 7091 2000 2171 1037 2261 2131 9436 1997 2031 2019 103 103 7166 2000 2012 1996 2168 2051 2017 2066 2000 2175 2000 2732 15890 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 2044 -100 -100 -100 22870 -100 -100 -100 -100 -100 2261 -100 -100 -100 -100 -100 3747 2006 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 1029 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 11:37:44 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   ********************************\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   ********************************\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 11:37:46 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:12<00:00, 817.20s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fc18f4eb536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fc1bfaaa183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fc1bfaacb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fc1bf8fab93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fc1bfd26db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fc1bf9032d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fc1bfd0d76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fc1bfd32c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fc1c170dceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fc1bfd32c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fc1c1515787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fc1c19f8c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fc1c19f5f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fc1c19f6ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fc1c19ef359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fc1ce12dcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fc1ced15e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fc1d0f256ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fc1d0c5b4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10/bt', sample_num=1, seed=10, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_10_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f5fe956a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f6019b29183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f6019b2bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f6019979b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f6019da5db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f60199822d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f6019d8c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f6019db1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f601b78cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f6019db1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f601b594787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f601ba77c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f601ba74f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f601ba75ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f601ba6e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f60281accf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f6028d94e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f602afa46ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f602acda4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 13:58:10 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 13:58:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 13:58:11 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 13:58:11 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 13:58:15 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 13:58:15 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   tokens: [CLS] description i ' ve heard about some interesting super ##sti ##tions about horseshoe ##s . [SEP]\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   init_ids: 101 6412 1045 1005 2310 2657 2055 2070 5875 3565 16643 9285 2055 23449 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   input_ids: 101 6412 1045 1005 2310 2657 103 103 5875 3565 2055 9285 2055 23449 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 2055 2070 -100 -100 16643 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   tokens: [CLS] description oh , i ' ve never heard about them before . [SEP]\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   init_ids: 101 6412 2821 1010 1045 1005 2310 2196 2657 2055 2068 2077 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   input_ids: 101 6412 2821 103 1045 1005 103 2196 2657 2055 2068 2077 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 1010 -100 -100 2310 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 13:58:15 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   tokens: [CLS] description all right , then . [SEP]\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   init_ids: 101 6412 2035 2157 1010 2059 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   input_ids: 101 6412 2035 2157 1010 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 2059 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   tokens: [CLS] description i gave you this month ' s allowance last week . [SEP]\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   init_ids: 101 6412 1045 2435 2017 2023 3204 1005 1055 21447 2197 2733 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   input_ids: 101 6412 1045 2435 103 2023 3204 1005 1055 21447 2017 2733 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2017 -100 -100 -100 -100 -100 2197 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 13:58:17 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   ********************************\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   ********************************\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 13:58:18 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:18:05<00:00, 828.58s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fa35b3bb536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fa38b97a183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fa38b97cb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fa38b7cab93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fa38bbf6db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fa38b7d32d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fa38bbdd76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fa38bc02c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fa38d5ddceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fa38bc02c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fa38d3e5787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fa38d8c8c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fa38d8c5f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fa38d8c6ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fa38d8bf359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fa399ffdcf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fa39abe5e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fa39cdf56ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fa39cb2b4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 282, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 278, in main\n",
      "    gen_eda(args.input, output, alpha=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 242, in gen_eda\n",
      "    aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 204, in eda\n",
      "    a_words = random_insertion(words, n_ri)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 163, in random_insertion\n",
      "    add_word(new_words)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 171, in add_word\n",
      "    random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/random.py\", line 222, in randint\n",
      "    return self.randrange(a, b+1)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/random.py\", line 200, in randrange\n",
      "    raise ValueError(\"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width))\n",
      "ValueError: empty range for randrange() (0,0, 0)\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10/bt', sample_num=1, seed=11, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_11_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f693de6a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f696e429183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f696e42bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f696e279b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f696e6a5db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f696e2822d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f696e68c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f696e6b1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f697008cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f696e6b1c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f696fe94787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f6970377c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f6970374f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f6970375ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f697036e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f697caaccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f697d694e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f697f8a46ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f697f5da4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 16:20:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 16:20:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 16:20:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/18/2021 16:20:42 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 16:20:46 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 16:20:46 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   tokens: [CLS] description what part did n ' t you like ? [SEP]\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   init_ids: 101 6412 2054 2112 2106 1050 1005 1056 2017 2066 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   input_ids: 101 6412 2054 2112 103 1050 1005 1056 103 2066 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2106 -100 -100 -100 2017 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   tokens: [CLS] description the plot was so predictable , and there were n ' t enough twists and turns . [SEP]\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   init_ids: 101 6412 1996 5436 2001 2061 21425 1010 1998 2045 2020 1050 1005 1056 2438 21438 1998 4332 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   input_ids: 101 6412 1996 5436 2001 2061 21425 1010 1998 2045 2020 103 1005 1056 2438 21425 1998 4332 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 1050 -100 -100 -100 21438 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 16:20:46 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   tokens: [CLS] description do you exercise regularly ? [SEP]\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   init_ids: 101 6412 2079 2017 6912 5570 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   input_ids: 101 6412 2079 2017 103 5570 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 6912 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   tokens: [CLS] description how about badminton ? [SEP]\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   init_ids: 101 6412 2129 2055 14618 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   input_ids: 101 6412 2129 103 14618 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 2055 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 16:20:48 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   ********************************\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   ********************************\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 16:20:49 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:16:47<00:00, 820.72s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fe73c9a6536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7fe76cf65183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7fe76cf67b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7fe76cdb5b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7fe76d1e1db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7fe76cdbe2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7fe76d1c876f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7fe76d1edc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7fe76ebc8ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7fe76d1edc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7fe76e9d0787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7fe76eeb3c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fe76eeb0f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fe76eeb1ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fe76eeaa359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fe77b5e8cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7fe77c1d0e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7fe77e3e06ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7fe77e1164dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 282, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 278, in main\n",
      "    gen_eda(args.input, output, alpha=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 242, in gen_eda\n",
      "    aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 217, in eda\n",
      "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 217, in <listcomp>\n",
      "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 55, in get_only_chars\n",
      "    if clean_line[0] == ' ':\n",
      "IndexError: string index out of range\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10/bt', sample_num=1, seed=12, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_12_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f95a2410536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f95d29cf183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f95d29d1b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f95d281fb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f95d2c4bdb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f95d28282d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f95d2c3276f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f95d2c57c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f95d4632ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f95d2c57c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f95d443a787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f95d491dc05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f95d491af03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f95d491bce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f95d4914359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f95e1052cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f95e1c3ae0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f95e3e4a6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f95e3b804dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "06/18/2021 18:41:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 18:41:52 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 18:41:52 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/18/2021 18:41:52 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 18:41:56 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 18:41:56 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   tokens: [CLS] description nine . [SEP]\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   init_ids: 101 6412 3157 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   input_ids: 101 6412 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   masked_lm_labels: -100 -100 3157 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   tokens: [CLS] description eight . [SEP]\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   init_ids: 101 6412 2809 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   input_ids: 101 6412 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   masked_lm_labels: -100 -100 2809 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 18:41:56 - INFO - __main__ -   label_len: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 18:41:58 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   tokens: [CLS] description in five years in ten years in twenty years i will study journalism and science at university . [SEP]\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   init_ids: 101 6412 1999 2274 2086 1999 2702 2086 1999 3174 2086 1045 2097 2817 8083 1998 2671 2012 2118 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   input_ids: 101 6412 1999 2274 2086 1999 2702 2086 1999 3174 2086 1045 2097 2817 8083 1998 2671 2012 2118 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 2671 -100 2118 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   tokens: [CLS] description i hope to master three foreign languages in ten years . [SEP]\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   init_ids: 101 6412 1045 3246 2000 3040 2093 3097 4155 1999 2702 2086 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   input_ids: 101 6412 1045 3246 2000 103 2093 3097 4155 1999 2702 2086 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 2000 3040 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 18:41:58 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   ********************************\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   ********************************\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 18:41:59 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:26:40<00:00, 880.08s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f2322fd1536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f2353590183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f2353592b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f23533e0b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f235380cdb7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f23533e92d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f23537f376f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f2353818c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f23551f3ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f2353818c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f2354ffb787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f23554dec05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f23554dbf03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f23554dcce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f23554d5359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f2361c13cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f23627fbe0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f2364a0b6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f23647414dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 282, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 278, in main\n",
      "    gen_eda(args.input, output, alpha=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 242, in gen_eda\n",
      "    aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 217, in eda\n",
      "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 217, in <listcomp>\n",
      "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/eda.py\", line 55, in get_only_chars\n",
      "    if clean_line[0] == ' ':\n",
      "IndexError: string index out of range\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10/bt', sample_num=1, seed=13, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_13_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f9d972cc536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f9dc788b183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f9dc788db64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f9dc76dbb93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f9dc7b07db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f9dc76e42d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f9dc7aee76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f9dc7b13c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f9dc94eeceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f9dc7b13c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f9dc92f6787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f9dc97d9c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f9dc97d6f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f9dc97d7ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f9dc97d0359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f9dd5f0ecf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f9dd6af6e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f9dd8d066ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f9dd8a3c4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 21:13:12 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 21:13:13 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 21:13:13 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/18/2021 21:13:13 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 21:13:17 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 21:13:17 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   tokens: [CLS] description sure , i enjoying myself . [SEP]\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   init_ids: 101 6412 2469 1010 1045 9107 2870 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   input_ids: 101 6412 2469 1010 1045 103 2870 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 9107 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   tokens: [CLS] description how ' s the dinner ? [SEP]\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   init_ids: 101 6412 2129 1005 1055 1996 4596 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   input_ids: 101 6412 2129 1005 1055 103 4596 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 1996 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 21:13:17 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   tokens: [CLS] description it is a reality . [SEP]\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   init_ids: 101 6412 2009 2003 1037 4507 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   input_ids: 101 6412 2009 103 1037 4507 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 2003 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   tokens: [CLS] description what ' s important is the creative process . [SEP]\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   init_ids: 101 6412 2054 1005 1055 2590 2003 1996 5541 2832 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   input_ids: 101 6412 2054 1005 1055 2590 2003 1996 5541 103 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 2832 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 21:13:18 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   ********************************\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   ********************************\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 21:13:20 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:15:20<00:00, 812.09s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f573adc5536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f576b384183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f576b386b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f576b1d4b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f576b600db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f576b1dd2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f576b5e776f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f576b60cc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f576cfe7ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f576b60cc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f576cdef787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f576d2d2c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f576d2cff03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f576d2d0ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f576d2c9359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f5779a07cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f577a5efe0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f577c7ff6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f577c5354dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "generated augmented sentences with eda for /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10/train.tsv to /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10/eda/eda_aug.tsv with num_aug=1\n",
      "usage: bert_classifier.py [-h] [--task {stsa,snips,trec}]\n",
      "                          [--data_dir DATA_DIR] [--seed SEED]\n",
      "                          [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]\n",
      "                          [--warmup_steps WARMUP_STEPS]\n",
      "                          [--max_seq_length MAX_SEQ_LENGTH] [--cache CACHE]\n",
      "                          [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                          [--learning_rate LEARNING_RATE]\n",
      "                          [--batch_size BATCH_SIZE]\n",
      "bert_classifier.py: error: argument --learning_rate: expected one argument\n",
      "Namespace(cache='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE', data_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10', gpu=0, output_dir='/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10/bt', sample_num=1, seed=14, task_name='trec', train_batch_size=32)\n",
      "Archive name '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was not found in archive name list. We assumed '/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/wmt19.en-de.joined-dict.single_model' was a path or URL but couldn't find any file associated to this path or URL.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 46, in main\n",
      "    backtranslation_using_en_de_model(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/backtranslation.py\", line 69, in backtranslation_using_en_de_model\n",
      "    bpe='fastbpe'\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 174, in from_pretrained\n",
      "    **kwargs,\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/fairseq/hub_utils.py\", line 51, in from_pretrained\n",
      "    kwargs['data'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/posixpath.py\", line 80, in join\n",
      "    a = os.fspath(a)\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "cat: /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/utils/datasets/L2/exp_14_10/bt/bt_aug.tsv: No such file or directory\n",
      "Training:   0%|                                         | 0/441 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f3331ac5536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f3362084183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f3362086b64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f3361ed4b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f3362300db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f3361edd2d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f33622e776f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f336230cc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f3363ce7ceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f336230cc76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f3363aef787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f3363fd2c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f3363fcff03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f3363fd0ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f3363fc9359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f3370707cf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f33712efe0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f33734ff6ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f33732354dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18/2021 23:32:59 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "06/18/2021 23:33:00 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "06/18/2021 23:33:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/18/2021 23:33:00 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/CACHE/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "06/18/2021 23:33:05 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "06/18/2021 23:33:05 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   guid: train-0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   tokens: [CLS] description you know , i often find it hard to start a conversation . [SEP]\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   init_ids: 101 6412 2017 2113 1010 1045 2411 2424 2009 2524 2000 2707 1037 4512 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   input_ids: 101 6412 2017 2113 1010 1045 2411 2424 2009 2524 2000 2707 1037 103 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 4512 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   guid: train-1\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   tokens: [CLS] description if i were you , i would act more friendly with your classmates . [SEP]\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   init_ids: 101 6412 2065 1045 2020 2017 1010 1045 2052 2552 2062 5379 2007 2115 19846 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   input_ids: 101 6412 103 1045 2020 2017 1010 1045 2052 2552 2062 103 103 2115 19846 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   masked_lm_labels: -100 -100 2065 -100 -100 -100 -100 -100 -100 -100 -100 5379 2007 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 23:33:05 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   guid: dev-0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   tokens: [CLS] description i met some children riding a wagon on their way home from school . [SEP]\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   init_ids: 101 6412 1045 2777 2070 2336 5559 1037 9540 2006 2037 2126 2188 2013 2082 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   input_ids: 101 6412 1045 2777 2070 2336 5559 1037 9540 103 2037 2126 2188 2013 103 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 -100 2006 -100 -100 -100 -100 2082 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   *** Example ***\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   guid: dev-1\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   tokens: [CLS] description but people can live there because of the oasis . [SEP]\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   init_ids: 101 6412 2021 2111 2064 2444 2045 2138 1997 1996 18128 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   input_ids: 101 6412 103 2111 2064 2444 2045 2138 1997 103 18128 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   masked_lm_labels: -100 -100 2021 -100 -100 -100 -100 -100 -100 1996 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "06/18/2021 23:33:06 - INFO - __main__ -   label_len: 1\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   ********************************\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   ***** [ Running training ] *****\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   ********************************\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   *  Num examples = 3522\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   *  Batch size = 8\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   *  Num steps = 4402\n",
      "06/18/2021 23:33:08 - INFO - __main__ -   *******************************\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "Epoch: 100%|█████████████████████████████████| 10/10 [2:20:51<00:00, 845.19s/it]\n",
      "/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/881 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 94, in <module>\n",
      "    main(args)\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_classifier.py\", line 48, in main\n",
      "    classifier.train_epoch()\n",
      "  File \"/home/delabgpu/Tae_StudyCode/TransformersDataAugmentation/src/bert_aug/bert_model.py\", line 69, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f302db4a536 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f305e109183 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::TensorIterator::build() + 0x44 (0x7f305e10bb64 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x193 (0x7f305df59b93 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: <unknown function> + 0x10b7db7 (0x7f305e385db7 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x172 (0x7f305df622d2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x109e76f (0x7f305e36c76f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: <unknown function> + 0x10c3c76 (0x7f305e391c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: <unknown function> + 0x2a9eceb (0x7f305fd6cceb in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #9: <unknown function> + 0x10c3c76 (0x7f305e391c76 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #10: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1f7 (0x7f305fb74787 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #11: <unknown function> + 0x2d89c05 (0x7f3060057c05 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #12: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f3060054f03 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #13: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f3060055ce2 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #14: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f306004e359 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #15: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f306c78ccf8 in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #16: <unknown function> + 0xee0f (0x7f306d374e0f in /home/delabgpu/anaconda3/envs/tae/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\n",
      "frame #17: <unknown function> + 0x76ba (0x7f306f5846ba in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #18: clone + 0x6d (0x7f306f2ba4dd in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash bert_L2_lower.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
